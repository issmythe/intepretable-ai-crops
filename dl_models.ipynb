{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/issmythe/intepretable-ai-crops/blob/main/dl_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gan7pByg7wRi"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-rSKk25BOni",
        "outputId": "23711dc8-9530-493a-c4c6-f2c247c77671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPJhT-L3jUSq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS1b1v2vmAIQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/current_research_projects/utils/')\n",
        "sys.path.append('drive/MyDrive/current_research_projects/dl_yield_forecasts/code/src/py/')\n",
        "\n",
        "import copy\n",
        "import importlib\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Analysis\n",
        "from datetime import datetime, timedelta\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.iolib.summary2 import summary_col\n",
        "\n",
        "# Plotting\n",
        "import matplotlib\n",
        "import plotly\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly import subplots\n",
        "\n",
        "# Utils\n",
        "from read_data import get_max_vi, get_modis_vi, get_sif, get_weather, get_yields\n",
        "from dl_helpers import fit_predict_fold, test\n",
        "from data_object import DataObj, DataObj2D, DataObjPretrain, BaggingDataObj, DummyDataObj\n",
        "\n",
        "from networks.network import Network\n",
        "from networks.basic_cnn import BasicCNN\n",
        "from networks.basic_lstm import BasicLSTM\n",
        "from networks.hybrid_lstm import HybridLSTM\n",
        "from networks.segmented_cnn import SegmentedCNN\n",
        "\n",
        "# Tensorflow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ONy6zJphxsOJ"
      },
      "outputs": [],
      "source": [
        "#@title Constants\n",
        "DATA_PATH = 'drive/MyDrive/current_research_projects/us_data/'\n",
        "DL_DATA_PATH = 'drive/MyDrive/current_research_projects/dl_yield_forecasts/data/'\n",
        "\n",
        "# HEAT_DATA_PATH = 'drive/MyDrive/research_ideas/heat_separability/data/'\n",
        "datestr = datetime.today().strftime('%Y%m')\n",
        "FIG_PATH = f'drive/MyDrive/current_research_projects/dl_yield_forecasts/figs/{datestr}/'\n",
        "! mkdir -p $FIG_PATH\n",
        "\n",
        "PROCESS_DATA = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDx8iPHM7lG4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGMl031a8i4k"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kKLv5tY9UdX"
      },
      "outputs": [],
      "source": [
        "PROCESS_DATA = False\n",
        "END_YEAR_INC = 2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k5ECcC1_8wWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1210b32-04bf-406e-9fde-413310622b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-993951d2b92b>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  yield_pre_preds['fips'] = yield_pre_preds['fips'].apply(lambda x: str(x).zfill(5))\n"
          ]
        }
      ],
      "source": [
        "#@title Get yields\n",
        "yields = get_yields(2000, END_YEAR_INC, True).drop('Unnamed: 0', axis=1)\n",
        "# yields.to_csv(DL_DATA_PATH + 'preprocessed_data/yields_east_2000_2021.csv', index=False)\n",
        "\n",
        "yield_pre_preds = pd.read_csv(f'{DL_DATA_PATH}/preprocessed_data/pretrain_labels.csv')\n",
        "yield_pre_preds['label_pred'] = yield_pre_preds.apply(\n",
        "    lambda x: [x[f'pred_{i + 1}'] for i in range(17)], axis=1)\n",
        "yield_pre_preds['label_delta'] = yield_pre_preds.apply(\n",
        "    lambda x: [x[f'delta_{i}'] for i in range(17)], axis=1)\n",
        "yield_pre_preds = yield_pre_preds[['fips', 'year', 'label_pred', 'label_delta']]\n",
        "yield_pre_preds['fips'] = yield_pre_preds['fips'].apply(lambda x: str(x).zfill(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3TKO7cJkT3ZM"
      },
      "outputs": [],
      "source": [
        "#@title Get ddays\n",
        "if False:\n",
        "    full_ddays = pd.read_csv(\n",
        "        f'{DATA_PATH}/weather/full_ddays_full_year/all_years_with_temp20240315.csv')\n",
        "\n",
        "    full_ddays = full_ddays.merge(\n",
        "        yields[['year']].assign(fips=yields['fips'].apply(lambda x: int(x))))\n",
        "\n",
        "    # Format data\n",
        "    full_ddays = full_ddays.sort_values('doy')\n",
        "    full_ddays['dday10C'] -= full_ddays['dday29C']\n",
        "    full_ddays = full_ddays[full_ddays['doy'] < 365]\n",
        "\n",
        "    # Make full-season dataframes\n",
        "    dday_cols = ['dday29C', 'dday10C', 'prec']\n",
        "    march_aug = full_ddays[(full_ddays['doy'] >= pd.to_datetime(f'2023-03-01').dayofyear) &\n",
        "                        (full_ddays['doy'] < pd.to_datetime(f'2023-09-01').dayofyear)]\\\n",
        "        .groupby(['fips', 'year'])[dday_cols].sum().reset_index()\n",
        "\n",
        "    april_sept = full_ddays[(full_ddays['doy'] >= pd.to_datetime(f'2023-04-01').dayofyear) &\n",
        "                            (full_ddays['doy'] < pd.to_datetime(f'2023-10-01').dayofyear)]\\\n",
        "        .groupby(['fips', 'year'])[dday_cols].sum().reset_index()\n",
        "\n",
        "    # Pivot and merge\n",
        "    weather_cols = dday_cols + ['tMin', 'tMax']\n",
        "    full_ddays = full_ddays[['fips', 'year', 'doy'] + weather_cols]\n",
        "    full_ddays = full_ddays.groupby(['fips', 'year']).agg({c: list for c in weather_cols}).reset_index()\n",
        "\n",
        "    daily_data = full_ddays.merge(\n",
        "        march_aug.rename({c: f'{c}_march_aug' for c in weather_cols}, axis=1)).merge(\n",
        "        april_sept.rename({c: f'{c}_april_sept' for c in weather_cols}, axis=1))\n",
        "\n",
        "    daily_data['fips'] = daily_data['fips'].apply(lambda x: str(x).zfill(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CMzQlBjHaiyq"
      },
      "outputs": [],
      "source": [
        "#@title Get EVI\n",
        "if False:\n",
        "    evi = pd.read_csv(DATA_PATH + 'evi/mean_cdl_2000_2021.csv')\n",
        "    evi['fips'] = evi['GEOID'].apply(lambda x: str(x).zfill(5))\n",
        "\n",
        "    evi['date'] = pd.to_datetime(evi['date'])\n",
        "    evi['year'] = evi['date'].dt.year\n",
        "    evi['doy'] = evi['date'].dt.dayofyear\n",
        "    evi = evi.merge(yields[['fips', 'year']])[['fips', 'year', 'date', 'doy', 'evi']]\n",
        "\n",
        "    start, end = 60, 365 - 31\n",
        "    all_dates = evi[['fips', 'year']].drop_duplicates().merge(pd.DataFrame(\n",
        "        {'doy': range(1, end)}), how='cross')\n",
        "    evi_all = evi.merge(all_dates, how='right')\n",
        "    interp = evi_all.groupby(['fips', 'year'])\\\n",
        "        .apply(lambda group: group['evi'].interpolate(method='linear'))\n",
        "    interp = interp.reset_index().drop('level_2', axis=1)\n",
        "    interp['doy'] = [x for x in range(1, end)] * len(interp[['fips', 'year']].drop_duplicates())\n",
        "    evi_all = evi_all.merge(interp.rename({'evi': 'evi_interp'}, axis=1))\n",
        "\n",
        "    evi_mini = evi_all[(evi_all['doy'] > start) & (evi_all['doy'] < end)]\n",
        "    evi_mini['idx'] = evi_all['doy'].apply(lambda x: (x - start) % 15)\n",
        "    evi_mini = evi_mini[evi_mini['idx'] == 0].groupby(['fips', 'year'])['evi_interp'].apply(list)\\\n",
        "        .reset_index().rename({'evi_interp': 'evi_mini'}, axis=1)\n",
        "    evi_all = evi_all[evi_all['doy'] >= start].groupby(['fips', 'year'])['evi_interp'].apply(list)\\\n",
        "        .reset_index().rename({'evi_interp': 'evi'}, axis=1)\n",
        "\n",
        "    daily_data = evi_all.merge(evi_mini).merge(daily_data)\n",
        "    daily_data.to_csv(\n",
        "        DL_DATA_PATH + 'preprocessed_data/prediction_df_through_2021_with_temp.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IZ1s0ho9qU55"
      },
      "outputs": [],
      "source": [
        "#@title Read pre-processed features\n",
        "daily_data = pd.read_csv(\n",
        "    DL_DATA_PATH + 'preprocessed_data/prediction_df_through_2021_with_temp.csv')\n",
        "daily_data['fips'] = daily_data['fips'].apply(lambda x: str(x).zfill(5))\n",
        "\n",
        "to_list = lambda x: [float(v) for v in x.strip('[').strip(']').split(', ')]\n",
        "for col in ['evi', 'evi_mini', 'dday10C', 'dday29C', 'prec', 'tMin', 'tMax']:\n",
        "    daily_data[col] = daily_data[col].apply(to_list)\n",
        "\n",
        "weather_cols = ['dday29C', 'dday10C', 'prec']\n",
        "daily_data = daily_data.rename({f'{c}_march_aug': f'{c}_total' for c in weather_cols}, axis=1)\\\n",
        "    .drop([f'{c}_april_sept' for c in weather_cols], axis=1)\n",
        "\n",
        "daily_data = daily_data.merge(yields).assign(reg_year=daily_data['year'] - daily_data['year'].min())\n",
        "daily_data = pd.get_dummies(daily_data, columns=['fips', 'state']).assign(\n",
        "    fips=daily_data['fips'], state=daily_data['state'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcN-No_6Rhn0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VFxlCLeelpRq"
      },
      "outputs": [],
      "source": [
        "#@title Filter and format pre-processed features\n",
        "def clip(df, var, thresh=0.0005):\n",
        "    return df[(df[var] >= df[var].quantile(thresh)) & (df[var] <= df[var].quantile(1 - thresh))]\n",
        "\n",
        "clipped_dd_29 = clip(daily_data, 'dday29C_total')\n",
        "clipped_dd_10 = clip(daily_data, 'dday10C_total')\n",
        "clipped_dd_prec = clip(daily_data, 'prec_total')\n",
        "\n",
        "feature_df = daily_data.merge(clipped_dd_29[['fips', 'year']])\\\n",
        "    .merge(clipped_dd_10[['fips', 'year']])\\\n",
        "    .merge(clipped_dd_prec[['fips', 'year']])\n",
        "\n",
        "for col in ['dday10C', 'dday29C', 'prec', 'tMin', 'tMax']:\n",
        "    feature_df[col] = feature_df[col].apply(lambda x: x[60:-30])\n",
        "    feature_df[f'{col}_total'] = feature_df[col].apply(sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYt2AafNWC5k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZnW49B-QyTe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get EVI pre-train labels\n",
        "evi_ts_df = pd.read_csv(f'{DL_DATA_PATH}/preprocessed_data/pretrain_evi.csv')\n",
        "evi_ts_df = evi_ts_df.groupby(['fips', 'year'])['evi_z'].apply(list).reset_index().merge(\n",
        "    evi_ts_df.groupby(['fips', 'year'])['evi_z_delta'].apply(list).reset_index())\n",
        "evi_ts_df['fips'] = evi_ts_df['fips'].apply(lambda x: str(x).zfill(5))\n",
        "\n",
        "feature_df = feature_df.merge(evi_ts_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDNCoZ80kVqj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ugeuPcPs9QT4"
      },
      "outputs": [],
      "source": [
        "#@title Constants - toggle between temp and ddays\n",
        "ts_cols = ['tMin', 'tMax', 'prec']\n",
        "\n",
        "state_cols = [x for x in feature_df.columns if x.startswith('state_')]\n",
        "fips_cols = [x for x in feature_df.columns if x.startswith('fips_')]\n",
        "year_cols = [\n",
        "    x for x in feature_df.columns if x.startswith('reg_year_') or x.startswith('reg_year2_')]\n",
        "flat_cols = ['reg_year'] + fips_cols\n",
        "\n",
        "random.seed(123)\n",
        "years = [x for x in range(2000, END_YEAR_INC + 1)]\n",
        "random.shuffle(years)\n",
        "FOLDS = np.array_split(years, 10)\n",
        "\n",
        "np.random.seed(123)\n",
        "train_years_gs = [x for x in range(2000, 2017)]\n",
        "np.random.shuffle(train_years_gs)\n",
        "GS_FOLDS = np.array_split(train_years_gs, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qr0orAM4-SVs"
      },
      "outputs": [],
      "source": [
        "#@title Eval helpers\n",
        "def get_rmse(c1, c2):\n",
        "    return ((c1 - c2) ** 2).mean() ** 0.5\n",
        "\n",
        "def get_corr(c1, c2):\n",
        "    return pearsonr(c1, c2)[0]\n",
        "\n",
        "def get_r2(c1, c2):\n",
        "    return pearsonr(c1, c2)[0] ** 2\n",
        "\n",
        "def get_lm_performance(fold, train=False):\n",
        "    def _get_stats(df):\n",
        "        if train:\n",
        "            sample_df = df[~df['year'].isin(FOLDS[fold])]\n",
        "        else:\n",
        "            sample_df = df[df['year'].isin(FOLDS[fold])]\n",
        "        return round(get_rmse(sample_df['pred'], sample_df['log_yield']), 3), \\\n",
        "               round(pearsonr(sample_df['pred'], sample_df['log_yield'])[0], 3)\n",
        "\n",
        "    print('Baseline RMSE, R:', _get_stats(lm_preds_bl))\n",
        "    print('March RMSE, R:', _get_stats(lm_preds_march))\n",
        "    print('April RMSE, R:', _get_stats(lm_preds_april))\n",
        "    print('USDA season RMSE, R:', _get_stats(lm_preds_season))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shared grid search helper\n",
        "def get_results_df(param_dict, all_preds, best_iters):\n",
        "    results_df = pd.DataFrame(param_dict, index=[0])\n",
        "    results_df['rmse'] = get_rmse(all_preds['log_yield'], all_preds['pred'])\n",
        "    results_df['corr'] = get_corr(all_preds['log_yield'], all_preds['pred'])\n",
        "    results_df['best_iters'] = [best_iters]\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "xeKpX-3m_wCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNpZygJd_x6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swnPpM3j8_jD",
        "outputId": "94480087-dc96-41ea-9b7c-73a66d267b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected N: 33064\n",
            "N = 33064 RMSE: 0.24 R2: 0.503\n",
            "N = 33064 RMSE: 0.208 R2: 0.626\n",
            "N = 33064 RMSE: 0.203 R2: 0.644\n",
            "N = 33064 RMSE: 0.201 R2: 0.65\n",
            "0.257 0.664\n",
            "0.219 0.769\n",
            "0.213 0.783\n",
            "0.212 0.786\n"
          ]
        }
      ],
      "source": [
        "#@title Read linear model results\n",
        "HEAT_DATA_PATH = 'drive/MyDrive/current_research_projects/heat_separability/data/'\n",
        "yield_lm_dir = f'{HEAT_DATA_PATH}/yield_predictions/'\n",
        "\n",
        "lm_preds_bl = pd.read_csv(f'{yield_lm_dir}/baseline_preds_31_states.csv')\n",
        "lm_preds_march = pd.read_csv(f'{yield_lm_dir}/march_through_aug_31_states.csv')\n",
        "lm_preds_april = pd.read_csv(f'{yield_lm_dir}/april_through_sept_31_states.csv')\n",
        "lm_preds_season = pd.read_csv(f'{yield_lm_dir}/doy_full_season_31_states.csv')\n",
        "\n",
        "def quick_summarize(preds):\n",
        "    rmse = get_rmse(preds['log_yield'], preds['pred'])\n",
        "    r2 = get_r2(preds['log_yield'], preds['pred'])\n",
        "    print('N =', len(preds), 'RMSE:', round(rmse, 3), 'R2:', round(r2, 3))\n",
        "\n",
        "print('Expected N:', len(yields))\n",
        "quick_summarize(lm_preds_bl)\n",
        "quick_summarize(lm_preds_april)\n",
        "quick_summarize(lm_preds_march)\n",
        "quick_summarize(lm_preds_season)\n",
        "\n",
        "for df in [lm_preds_bl, lm_preds_april, lm_preds_march, lm_preds_season]:\n",
        "    train_df = df[df['year'] < 2017]\n",
        "    print(round(get_rmse(train_df['log_yield'], train_df['pred']), 3),\n",
        "          round(get_corr(train_df['log_yield'], train_df['pred']), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nw6Gx06HoWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c499369-bc29-43de-983e-e0dcecf51d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/preprocessed_data/pretrain_evi.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! mv $DL_DATA_PATH2/preprocessed_data/pretrain_evi.csv $DL_DATA_PATH/preprocessed_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M29XpBXaennI"
      },
      "source": [
        "# EVI exploratory analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper to calculate z-scores\n",
        "def get_z_score_data(df, col):\n",
        "    mean_fn = lambda g: np.mean(np.stack(g[col].values), axis=0)\n",
        "    std_fn = lambda g: np.std(np.stack(g[col].values), axis=0)\n",
        "\n",
        "    z_data = df[df['year'] <= 2016]\n",
        "    mean_ts = z_data.groupby(['fips']).apply(mean_fn).reset_index()\n",
        "    std_ts = z_data.groupby(['fips']).apply(std_fn).reset_index()\n",
        "    state_mean_ts = z_data.groupby(['state']).apply(mean_fn).reset_index()\n",
        "    state_std_ts = z_data.groupby(['state']).apply(std_fn).reset_index()\n",
        "\n",
        "    zdf = df[['fips', 'state', 'year', col]].merge(\n",
        "        mean_ts.rename({0: f'county_mean_{col}'}, axis=1), how='left').merge(\n",
        "        std_ts.rename({0: f'county_std_{col}'}, axis=1), how='left').merge(\n",
        "        state_mean_ts.rename({0: f'state_mean_{col}'}, axis=1), how='left').merge(\n",
        "        state_std_ts.rename({0: f'state_std_{col}'}, axis=1), how='left')\n",
        "\n",
        "    zdf[f'county_mean_{col}'] = zdf[f'county_mean_{col}'].fillna(zdf[f'state_mean_{col}'])\n",
        "    zdf[f'county_std_{col}'] = zdf[f'state_std_{col}'].fillna(zdf[f'state_mean_{col}'])\n",
        "    zdf[f'{col}_z'] = np.divide(\n",
        "        np.subtract(zdf[f'{col}'], zdf[f'county_mean_{col}']), zdf[f'county_std_{col}'])\\\n",
        "        .apply(lambda x: np.nan_to_num(x, posinf=0))\n",
        "    return zdf[['fips', 'state', 'year', col, f'{col}_z']]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1xwCmzfdaOhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAdl2hSto5-N",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get EVI df\n",
        "n_periods, n_days, shift_days = 17, 17, 16\n",
        "\n",
        "# Get z-score data\n",
        "evi_ts_df = get_z_score_data(feature_df, 'evi')\n",
        "\n",
        "# Explode df to get EVI variables for end of each period\n",
        "def _index_evi(i):\n",
        "    return evi_ts_df[['fips', 'state', 'year']].assign(\n",
        "        evi=evi_ts_df['evi'].apply(lambda x: x[shift_days * i + n_days]),\n",
        "        evi_z=evi_ts_df['evi_z'].apply(lambda x: x[shift_days * i + n_days]),\n",
        "        idx=i)\n",
        "\n",
        "start_evi = evi_ts_df[['fips', 'state', 'year']].assign(\n",
        "    evi=evi_ts_df['evi'].apply(lambda x: x[0]),\n",
        "    evi_z=evi_ts_df['evi_z'].apply(lambda x: x[0]),\n",
        "    idx=-1)\n",
        "evi_df = pd.concat([start_evi] + [_index_evi(i) for i in range(n_periods)])\n",
        "\n",
        "# Add delta variables and yield data\n",
        "evi_df['evi_prev'] = evi_df.groupby(['fips', 'year'])['evi'].shift(1)\n",
        "evi_df['evi_z_prev'] = evi_df.groupby(['fips', 'year'])['evi_z'].shift(1)\n",
        "evi_df['evi_delta'] = evi_df['evi'] - evi_df['evi_prev']\n",
        "evi_df['evi_z_delta'] = evi_df['evi_z'] - evi_df['evi_z_prev']\n",
        "evi_df = evi_df[evi_df['idx'] >= 0].merge(yields)\n",
        "\n",
        "# Get correlation for each period\n",
        "evi_corr_df = evi_df.groupby('idx')\\\n",
        "    [['log_yield', 'evi', 'evi_z', 'evi_delta', 'evi_z_delta']].corr().reset_index()\n",
        "evi_corr_df = evi_corr_df[evi_corr_df['level_1'] == 'log_yield'].drop(\n",
        "    ['level_1', 'log_yield'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PcWZDzP8oTLs"
      },
      "outputs": [],
      "source": [
        "#@title Plot EVI vs yield over time\n",
        "fig = plotly.subplots.make_subplots(rows=2, cols=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(name='EVI', x=evi_corr_df['idx'], y=evi_corr_df['evi']), row=1, col=1)\n",
        "fig.add_trace(go.Scatter(name='∆EVI', x=evi_corr_df['idx'], y=evi_corr_df['evi_delta']))\n",
        "fig.add_trace(go.Scatter(name='EVI z-score', x=evi_corr_df['idx'], y=evi_corr_df['evi_z']))\n",
        "fig.add_trace(go.Scatter(name='∆EVI z-score', x=evi_corr_df['idx'], y=evi_corr_df['evi_z_delta']))\n",
        "\n",
        "fig.update_yaxes(title='Correlation with log yield', row=1, col=1)\n",
        "fig.update_xaxes(title='Time step', row=1, col=1)\n",
        "\n",
        "mean_evi_ts = evi_df.groupby('idx')['evi'].mean().reset_index()\n",
        "\n",
        "fig.add_trace(go.Scatter(name='EVI', x=mean_evi_ts['idx'], y=mean_evi_ts['evi']), row=2, col=1)\n",
        "\n",
        "fig.update_layout(height=800, width=800)\n",
        "# plotly.io.write_image(fig, f'{FIG_PATH}/evi_vs_yield.png', scale=2)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MFTQJ5sK9FK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lVp5xD0hzVan"
      },
      "outputs": [],
      "source": [
        "#@title Get baseline yield predictions\n",
        "state_cols = [x for x in feature_df.columns if x.startswith('state_')]\n",
        "\n",
        "for c in state_cols:\n",
        "    feature_df[f'year_{c}'] = feature_df[c] * feature_df['reg_year']\n",
        "    feature_df[f'year_{c}_2'] = feature_df[c] * feature_df['reg_year'] ** 2\n",
        "\n",
        "year_cols = [x for x in feature_df.columns if x.startswith('year_state_')]\n",
        "\n",
        "train_df = feature_df[feature_df['year'] <= 2016]\n",
        "train_fips = train_df[flat_cols].sum().reset_index()\n",
        "train_fips = train_fips[train_fips[0] > 0]['index'].to_list()\n",
        "\n",
        "baseline_model = linear_model.LinearRegression().fit(\n",
        "    y=train_df['log_yield'], X=train_df[train_fips + year_cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxw7OgNXAhpD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "dXaUWLbupGg4"
      },
      "outputs": [],
      "source": [
        "#@title EVI vs. yield over time\n",
        "baseline_preds = train_df[['fips', 'year', 'log_yield']].assign(\n",
        "    pred_0=baseline_model.predict(train_df[train_fips + year_cols]))\n",
        "\n",
        "evi_var = 'evi_z'\n",
        "for i in range(17):\n",
        "    df_i = baseline_preds.merge(evi_df[evi_df['idx'] == i])\n",
        "    model_i = linear_model.LinearRegression().fit(y=df_i['log_yield'], X=df_i[[evi_var, f'pred_{i}']])\n",
        "    baseline_preds = baseline_preds.assign(\n",
        "        **{f'pred_{i + 1}': model_i.predict(df_i[[evi_var, f'pred_{i}']])})\n",
        "    rmse = get_rmse(baseline_preds['log_yield'], baseline_preds[f'pred_{i + 1}'])\n",
        "    corr = get_corr(baseline_preds['log_yield'], baseline_preds[f'pred_{i + 1}'])\n",
        "\n",
        "    print(i, round(rmse, 3), round(corr, 3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P1dRUKMe1waG"
      },
      "outputs": [],
      "source": [
        "#@title EVI vs. yield residual over time\n",
        "\n",
        "def get_rmse_corr(evi_var, verbose):\n",
        "    baseline_preds = feature_df[['fips', 'year', 'log_yield']].assign(\n",
        "        pred_0=baseline_model.predict(feature_df[train_fips + year_cols]))\n",
        "    baseline_preds['residual'] = baseline_preds['log_yield'] - baseline_preds['pred_0']\n",
        "    baseline_preds['pred_0'] = 0\n",
        "\n",
        "    corrs, rmses = [], []\n",
        "    for i in range(17):\n",
        "        df_i = baseline_preds.merge(evi_df[evi_df['idx'] == i])\n",
        "        train_i = df_i[df_i['year'] <= 2016]\n",
        "\n",
        "        model_i = linear_model.LinearRegression().fit(\n",
        "            y=train_i['residual'], X=train_i[[evi_var, f'pred_{i}']])\n",
        "        baseline_preds = baseline_preds.assign(\n",
        "            **{f'pred_{i + 1}': model_i.predict(df_i[[evi_var, f'pred_{i}']])})\n",
        "\n",
        "        eval_preds = baseline_preds[baseline_preds['year'] <= 2016]\n",
        "        rmse = get_rmse(eval_preds['residual'], eval_preds[f'pred_{i + 1}'])\n",
        "        corr = get_corr(eval_preds['residual'], eval_preds[f'pred_{i + 1}'])\n",
        "        rmses.append(rmse)\n",
        "        corrs.append(corr)\n",
        "        if verbose:\n",
        "            print(i, round(rmse, 3), round(corr, 3))\n",
        "\n",
        "    return rmses, corrs, baseline_preds\n",
        "\n",
        "rmse_evi_z, corr_evi_z, yield_pre_preds = get_rmse_corr('evi_z', False)\n",
        "rmse_evi, corr_evi, _ = get_rmse_corr('evi', False)\n",
        "# yield_pre_preds.to_csv(f'{DL_DATA_PATH}/preprocessed_data/pretrain_labels.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FGatlpv4sPu7"
      },
      "outputs": [],
      "source": [
        "#@title Plot EVI vs yield over time\n",
        "fig = plotly.subplots.make_subplots(rows=1, cols=2)\n",
        "\n",
        "x = [x for x in range(17)]\n",
        "fig.add_trace(go.Scatter(name='EVI', x=x, y=rmse_evi, line_color='red'), row=1, col=1)\n",
        "fig.add_trace(go.Scatter(name='EVI z-score', x=x, y=rmse_evi_z, line_color='blue'), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(showlegend=False, x=x, y=corr_evi, line_color='red'), row=1, col=2)\n",
        "fig.add_trace(go.Scatter(showlegend=False, x=x, y=corr_evi_z, line_color='blue'), row=1, col=2)\n",
        "\n",
        "\n",
        "fig.update_yaxes(title='RMSE vs. yield residual', row=1, col=2)\n",
        "fig.update_yaxes(title='Correlation vs. yield residual', row=1, col=2)\n",
        "fig.update_xaxes(title='Time step')\n",
        "\n",
        "\n",
        "fig.update_layout(height=600, width=1000)\n",
        "# plotly.io.write_image(fig, f'{FIG_PATH}/evi_vars_vs_yield_residual.png', scale=2)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Yujk3Gss88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "INhF7y8_PSjC"
      },
      "outputs": [],
      "source": [
        "# @title Make weather df\n",
        "# Get z scores for weather vars\n",
        "prec_df = get_z_score_data(feature_df, 'prec')\n",
        "tmin_df = get_z_score_data(feature_df, 'tMin')\n",
        "tmax_df = get_z_score_data(feature_df, 'tMax')\n",
        "weather_z = prec_df.merge(tmin_df).merge(tmax_df)\n",
        "eval_df = weather_z[weather_z['year'] <= 2016]\n",
        "\n",
        "# Split by time period\n",
        "def index_array(x, idx):\n",
        "    return x[shift_days * idx: shift_days * idx + n_days]\n",
        "\n",
        "eval_df = pd.concat([\n",
        "    eval_df.assign(**{c: eval_df[c].apply(lambda x: index_array(x, i)) for c in ts_cols})\n",
        "    .assign(**{f'{c}_z': eval_df[f'{c}_z'].apply(lambda x: index_array(x, i)) for c in ts_cols})\n",
        "    .assign(idx=i) for i in range(n_periods)])\n",
        "\n",
        "# Get average weather var values by time period\n",
        "for col in ts_cols:\n",
        "    eval_df[f'{col}_avg'] = eval_df[col].apply(np.mean)\n",
        "    eval_df[f'{col}_z_avg'] = eval_df[f'{col}_z'].apply(np.mean)\n",
        "\n",
        "\n",
        "# Add yield data\n",
        "def get_yield_labels(i):\n",
        "    return yield_pre_preds[['fips', 'year', f'pred_{i + 1}', f'delta_{i}']].rename(\n",
        "        {f'pred_{i + 1}': f'pred', f'delta_{i}': 'delta' }, axis=1).assign(idx=i)\n",
        "\n",
        "for i in range(17):\n",
        "    yield_pre_preds[f'delta_{i}'] = yield_pre_preds[f'pred_{i + 1}'] - yield_pre_preds[f'pred_{i}']\n",
        "\n",
        "eval_df = eval_df.drop(ts_cols + [f'{c}_z' for c in ts_cols], axis=1).merge(\n",
        "    pd.concat([get_yield_labels(i) for i in range(17)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ecAjTeOxI9b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V68Cuv4HoTFC"
      },
      "outputs": [],
      "source": [
        "#@title Predict pre-train variables with average weather by period\n",
        "\n",
        "def _fit_predict(col_fn, y_var, i, df=None):\n",
        "    if i is not None:\n",
        "        pdf = eval_df[eval_df['idx'] == i]\n",
        "    else:\n",
        "        pdf = eval_df\n",
        "\n",
        "    train = pdf[~pdf['year'].isin(FOLDS[0])]\n",
        "    test = pdf[pdf['year'].isin(FOLDS[0])]\n",
        "\n",
        "    preds = linear_model.LinearRegression().fit(\n",
        "        X=train[[col_fn(c) for c in ts_cols]], y=train[y_var]).predict(\n",
        "        test[[col_fn(c) for c in ts_cols]])\n",
        "    return get_rmse(preds, test[y_var]), get_corr(preds, test[y_var])\n",
        "\n",
        "result_dfs = []\n",
        "\n",
        "for i in range(17):\n",
        "    rmse_pred, r_pred = _fit_predict(lambda c: f'{c}_avg', 'pred', i)\n",
        "    rmse_pred_z, r_pred_z = _fit_predict(lambda c: f'{c}_z_avg', 'pred', i)\n",
        "    rmse_delta, r_delta = _fit_predict(lambda c: f'{c}_avg', 'delta', i)\n",
        "    rmse_delta_z, r_delta_z = _fit_predict(lambda c: f'{c}_z_avg', 'delta', i)\n",
        "\n",
        "    i_df = dict(idx=i, rmse_pred=rmse_pred, r_pred=r_pred,\n",
        "                rmse_pred_z=rmse_pred_z, r_pred_z=r_pred_z,\n",
        "                rmse_delta=rmse_delta, r_delta=r_delta,\n",
        "                rmse_delta_z=rmse_delta_z, r_delta_z=r_delta_z)\n",
        "    result_dfs.append(pd.DataFrame(i_df, index=[0]))\n",
        "\n",
        "weather_to_yield = pd.concat(result_dfs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fJ4dfQiAwP8g"
      },
      "outputs": [],
      "source": [
        "#@title Predict pre-train variables with average weather, pooled\n",
        "_, r_pred = _fit_predict(lambda c: f'{c}_avg', 'pred', i=None)\n",
        "_, r_pred_z = _fit_predict(lambda c: f'{c}_z_avg', 'pred', i=None)\n",
        "_, r_delta = _fit_predict(lambda c: f'{c}_avg', 'delta', i=None)\n",
        "_, r_delta_z = _fit_predict(lambda c: f'{c}_z_avg', 'delta', i=None)\n",
        "print(round(r_pred, 3), round(r_pred_z, 3), round(r_delta, 3), round(r_delta_z, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t0TZPJPVrI4u"
      },
      "outputs": [],
      "source": [
        "#@title Plot average weather vs. pre-train variables\n",
        "fig = plotly.subplots.make_subplots()\n",
        "\n",
        "x = weather_to_yield['idx']\n",
        "\n",
        "fig.add_trace(go.Scatter(name='Weather vs. pred', x=x, y=weather_to_yield['r_pred']))\n",
        "fig.add_trace(go.Scatter(name='Weather z-score vs. pred', x=x, y=weather_to_yield['r_pred_z']))\n",
        "fig.add_trace(go.Scatter(name='Weather vs. delta', x=x, y=weather_to_yield['r_delta']))\n",
        "fig.add_trace(go.Scatter(name='Weather z-score vs. delta', x=x, y=weather_to_yield['r_delta_z']))\n",
        "\n",
        "fig.update_yaxes(title='Correlation')\n",
        "fig.update_xaxes(title='Time step')\n",
        "\n",
        "\n",
        "fig.update_layout(height=600, width=1000)\n",
        "# plotly.io.write_image(fig, f'{FIG_PATH}/weather_vs_pretraining_labels.png', scale=2)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVVOggZEwP3u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQZDYuiAAVd6"
      },
      "source": [
        "# Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI7oqz1pKaSR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Grid search helpers\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def do_gs_cv(param_dict, ts_cols, flat_cols, init_model_fn):\n",
        "    all_preds, best_iters = [], []\n",
        "    t0 = time.time()\n",
        "    for i in range(len(GS_FOLDS)):\n",
        "        model = init_model_fn(use_best_params=False, kwargs=param_dict)\n",
        "        model.to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=param_dict['lr'])\n",
        "\n",
        "        do = DataObj(feature_df, GS_FOLDS[i], GS_FOLDS[i], train_years_gs, ts_cols, flat_cols)\n",
        "        print(do.train_pd['year'].unique(), do.test_pd['year'].unique(), do.val_pd['year'].unique())\n",
        "\n",
        "        preds, best_iter = fit_predict_fold(do, ts_cols, flat_cols, model, optimizer,\n",
        "            param_dict['n_epochs'], device, criterion, verbose=False)\n",
        "        all_preds.append(preds), best_iters.append(best_iter[0])\n",
        "        print(f'CV fold {i + 1} / {len(GS_FOLDS)} complete, {round(time.time() - t0)} elapsed')\n",
        "\n",
        "    return get_results_df(param_dict, pd.concat(all_preds), best_iters)\n",
        "\n",
        "\n",
        "def make_param_combos(param_grid):\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    param_combos = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    np.random.seed(123)\n",
        "    param_combos = np.random.choice(param_combos, len(param_combos), replace=False)\n",
        "    return [x for x in param_combos if not (x['k'] == 9 and x['stride'] == 8)]\n",
        "\n",
        "\n",
        "def do_grid_search(network, use_ddays, param_grid=None, start=0, end=100):\n",
        "    dummy_network = network(True)\n",
        "    data_str = 'ddays' if use_ddays else 'temperature'\n",
        "    out_dir = f'{DL_DATA_PATH}/grid_search/{dummy_network.network_name}_{data_str}'\n",
        "    ! mkdir $out_dir\n",
        "\n",
        "    param_grid = param_grid if param_grid else dummy_network.get_param_grid()\n",
        "    param_combos = make_param_combos(param_grid)\n",
        "    ts_cols = ['dday10C', 'dday29C', 'prec'] if use_ddays else ['tMin', 'tMax', 'prec']\n",
        "    flat_cols = ['reg_year'] + fips_cols\n",
        "\n",
        "    print(ts_cols, set(x.split('_')[0] for x in flat_cols))\n",
        "    date_str = datetime.today().strftime('%Y%m%d')\n",
        "\n",
        "    results = []\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i in range(start, end):\n",
        "        iter_results = do_gs_cv(param_combos[i], ts_cols, flat_cols, network).assign(param_combo=i)\n",
        "        iter_results.to_csv(f'{out_dir}/{date_str}_{i}.csv', index=False)\n",
        "        print(iter_results)\n",
        "        print(f'Param combo {i} complete, {round((time.time() - t0) / 60, 1)} elapsed')\n",
        "\n",
        "    pd.concat(results).to_csv(f'{out_dir}/{date_str}_{i}.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR29MHJ87O0o",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Do grid search\n",
        "from networks.basic_cnn import BasicCNN\n",
        "do_grid_search(BasicCNN, use_ddays=False, end=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHCo3fgJwkke"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eergrscIffZU"
      },
      "source": [
        "# Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9H9ePmGuKgf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Helper\n",
        "N_BAGGING_FOLDS = 100\n",
        "\n",
        "def do_bagging(network, use_ddays, df=feature_df, ts_cols=None, start=0, end=100):\n",
        "    if ts_cols is None:\n",
        "        ts_cols = ['dday10C', 'dday29C', 'prec'] if use_ddays else ['tMin', 'tMax', 'prec']\n",
        "    data_str = 'dday' if use_ddays else 'temperature'\n",
        "    name = network(use_best_params=True, use_ddays=use_ddays).network_name\n",
        "    out_dir = f'{DL_DATA_PATH}/bagging/{name}_{data_str}'\n",
        "    print('*', out_dir)\n",
        "    ! mkdir $out_dir\n",
        "    test_years = [x for x in range(2017, END_YEAR_INC + 1)]\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    t0 = time.time()\n",
        "    for i in range(start, end):\n",
        "        print(i)\n",
        "        model = network(use_best_params=True, use_ddays=use_ddays, read_params=False)\n",
        "        if i == 0:\n",
        "            print(model.best_kwargs)\n",
        "        model.to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=model.get_best_param('lr'))\n",
        "        do = BaggingDataObj(df, test_years, i + 1, ts_cols, flat_cols)\n",
        "\n",
        "        preds, _ = fit_predict_fold(do, ts_cols, flat_cols, model, optimizer,\n",
        "                                    model.get_best_param('n_epochs'), device,\n",
        "                                    criterion, verbose=False)\n",
        "        # preds.to_csv(f'{out_dir}/{i}.csv', index=False)\n",
        "\n",
        "        r = get_corr(preds['pred'], preds['log_yield'])\n",
        "        rmse = get_rmse(preds['pred'], preds['log_yield'])\n",
        "        print(f'Bagging fold {i + 1}/{N_BAGGING_FOLDS} complete, {round(time.time() - t0)} elapsed',\n",
        "              round(r, 3), round(rmse, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fg5b85E7rQD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Do bagging\n",
        "# Framework for models other than HybridLSTM\n",
        "do_bagging(BasicCNN, use_ddays=False, start=78, end=79)\n",
        "\n",
        "# Framework for HybridLSTM\n",
        "from networks.hybrid_lstm import HybridLSTM\n",
        "\n",
        "pretrain_loc = 2\n",
        "def init_hlstm(use_best_params, kwargs=None, use_ddays=True, read_params=False,\n",
        "               pretrain_loc=pretrain_loc, verbose=False):\n",
        "    return HybridLSTM(use_best_params, kwargs=kwargs, use_ddays=use_ddays, read_params=read_params,\n",
        "                 pretrain_loc=pretrain_loc, pretrain=False, verbose=verbose)\n",
        "\n",
        "do_bagging(init_hlstm, False, start=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXfT56fdHpxq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feLIkLnAmBPt"
      },
      "source": [
        "# Pretrain workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IkVu0GZylAtl"
      },
      "outputs": [],
      "source": [
        "#@title Helpers\n",
        "y_col = 'evi_z'\n",
        "\n",
        "def set_gradients(model, freeze_layers, unfreeze):\n",
        "    for name, param in model.named_parameters():\n",
        "            if name in freeze_layers:\n",
        "                param.requires_grad = unfreeze\n",
        "            else:\n",
        "                break\n",
        "\n",
        "def get_freeze_layers(pretrain_loc):\n",
        "    freeze_layers = ['conv2d.weight', 'conv2d.bias']\n",
        "    if pretrain_loc == 1:\n",
        "        freeze_layers += ['fc_pretrain.weight', 'fc_pretrain.bias']\n",
        "    if pretrain_loc >= 2:\n",
        "        freeze_layers += [\n",
        "            'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0']\n",
        "    if pretrain_loc == 3:\n",
        "        freeze_layers += ['fc1.weight', 'fc1.bias']\n",
        "    return freeze_layers\n",
        "\n",
        "\n",
        "def fit_predict_pretrain_fold(pretrain_do, full_do, pretrain_loc, epochs, lrs, device,\n",
        "                              criterion, verbose=False, return_model=False):\n",
        "    print(ts_cols)\n",
        "    model = HybridLSTM(use_best_params=True, use_ddays=False, read_params=False,\n",
        "                       verbose=False, pretrain_loc=pretrain_loc, pretrain=True)\n",
        "    model.to(device)\n",
        "    freeze_layers = get_freeze_layers(pretrain_loc)\n",
        "    n_epochs_pt1, n_epochs_pt2, n_epochs_combined = epochs\n",
        "    lr1, lr2, lr_combo = lrs\n",
        "\n",
        "    # Train first half\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
        "    preds, _ = fit_predict_fold(pretrain_do, ts_cols, flat_cols, model, optimizer, n_epochs_pt1,\n",
        "                                device, criterion, verbose=verbose, print_train=False)\n",
        "\n",
        "    # Freeze first half + train second half\n",
        "    set_gradients(model, freeze_layers, False)\n",
        "    model.pretrain = False\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
        "    preds, _ = fit_predict_fold(full_do, ts_cols, flat_cols, model, optimizer, n_epochs_pt2, device,\n",
        "                                criterion, verbose=verbose, print_train=False)\n",
        "\n",
        "    # Train jointly & predict\n",
        "    set_gradients(model, freeze_layers, True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr_combo)\n",
        "    preds, _ = fit_predict_fold(full_do, ts_cols, flat_cols, model, optimizer, n_epochs_combined,\n",
        "                                device, criterion, verbose=verbose, print_train=False)\n",
        "    if return_model:\n",
        "        return preds, model\n",
        "    else:\n",
        "        return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh-pVRxCoEqp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVRdJU_Ok8C9"
      },
      "source": [
        "## **Train sample network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kT4r1wcouOlu"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "pretrain_loc = 3\n",
        "freeze_layers = get_freeze_layers(pretrain_loc)\n",
        "\n",
        "n_epochs_pt1, n_epochs_pt2, n_epochs_combined = 20, 10, 10\n",
        "lr1, lr2, lr_combo = 1e-4, 1e-4, 5e-5\n",
        "\n",
        "pretrain_do = DataObj(feature_df, FOLDS[0], FOLDS[0], None, ts_cols, flat_cols, y=y_col)\n",
        "full_do = DataObj(feature_df, FOLDS[0], FOLDS[0], None, ts_cols, flat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EzUr6EOmcITq"
      },
      "outputs": [],
      "source": [
        "#@title Train\n",
        "verbose = True\n",
        "model = HybridLSTM(use_best_params=True, use_ddays=False, read_params=False,\n",
        "                    verbose=False, pretrain_loc=pretrain_loc, pretrain=True)\n",
        "model.to(device)\n",
        "\n",
        "# Train first half\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
        "preds, _ = fit_predict_fold(pretrain_do, ts_cols, flat_cols, model, optimizer, n_epochs_pt1, device,\n",
        "                    criterion, verbose=verbose, print_train=False)\n",
        "\n",
        "# Freeze first half + train second half\n",
        "set_gradients(model, freeze_layers, False)\n",
        "model.pretrain = False\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
        "preds, _ = fit_predict_fold(full_do, ts_cols, flat_cols, model, optimizer, n_epochs_pt2, device,\n",
        "                    criterion, verbose=verbose, print_train=False)\n",
        "\n",
        "# Train jointly & predict\n",
        "set_gradients(model, freeze_layers, True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr_combo)\n",
        "preds, _ = fit_predict_fold(full_do, ts_cols, flat_cols, model, optimizer, n_epochs_combined,\n",
        "                            device, criterion, verbose=verbose, print_train=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxmXvI64YuWd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnsnVTjFlGIT"
      },
      "source": [
        "##** Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwEk7Hlmmb-m",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Helpers\n",
        "param_grid = dict(n_epochs_pt1=[10, 20, 35, 50],\n",
        "                  n_epochs_pt2=[10, 20, 35, 50],\n",
        "                  n_epochs_combined=[5, 10, 20, 35],\n",
        "                  lr1=[5e-5, 7.5e-5, 1e-4],\n",
        "                  lr2=[5e-5, 7.5e-5, 1e-4],\n",
        "                  lr_combo=[2.5e-5, 5e-5, 7.5e-5])\n",
        "\n",
        "def make_param_combos_pt(param_grid):\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    param_combos = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "    np.random.seed(123)\n",
        "    pcs = np.random.choice(param_combos, len(param_combos), replace=False)\n",
        "    pcs = [x for x in pcs if x['lr_combo'] < min(x['lr1'], x['lr2'])]\n",
        "    return [x for x in pcs if x['n_epochs_combined'] <= min(x['n_epochs_pt1'], x['n_epochs_pt2'])]\n",
        "\n",
        "\n",
        "def extract_params(param_combos, fold):\n",
        "    p = param_combos[fold]\n",
        "    epochs = (p['n_epochs_pt1'], p['n_epochs_pt2'], p['n_epochs_combined'])\n",
        "    lr = (p['lr1'], p['lr2'], p['lr_combo'])\n",
        "    return epochs, lr\n",
        "\n",
        "\n",
        "def do_gs_cv_pt(param_combos, fold, ts_cols, flat_cols, pretrain_loc, device, criterion):\n",
        "    epochs, lrs = extract_params(param_combos, fold)\n",
        "\n",
        "    all_preds = []\n",
        "    t0 = time.time()\n",
        "    for i in range(len(GS_FOLDS)):\n",
        "        pretrain_do = DataObj(\n",
        "            feature_df, GS_FOLDS[i], GS_FOLDS[i], train_years_gs, ts_cols, flat_cols, y=y_col)\n",
        "        full_do = DataObj(feature_df, GS_FOLDS[i], GS_FOLDS[i], train_years_gs, ts_cols, flat_cols)\n",
        "\n",
        "        preds = fit_predict_pretrain_fold(\n",
        "            pretrain_do, full_do, pretrain_loc, epochs, lrs, device, criterion, verbose=False)\n",
        "        all_preds.append(preds)\n",
        "\n",
        "        print(f'CV fold {i + 1} / {len(GS_FOLDS)} complete, {round(time.time() - t0)} elapsed')\n",
        "\n",
        "    return get_results_df(param_combos[fold], pd.concat(all_preds))\n",
        "\n",
        "\n",
        "def do_grid_search(pretrain_loc, param_grid, start=0, end=100):\n",
        "    out_dir = f'{DL_DATA_PATH}/grid_search/pretrain_{pretrain_loc}_temperature'\n",
        "    print('*', out_dir)\n",
        "    ! mkdir $out_dir\n",
        "\n",
        "    param_combos = make_param_combos_pt(param_grid)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    t0 = time.time()\n",
        "    for fold in range(start, end):\n",
        "        # pre-train is set up to only run with tmin, tmax, prec as ts_cols\n",
        "        iter_results = do_gs_cv_pt(\n",
        "            param_combos, fold, ts_cols, flat_cols, pretrain_loc, device, criterion)\n",
        "        # iter_results.assign(param_combo=fold).to_csv(f'{out_dir}/{fold}.csv', index=False)\n",
        "        print(iter_results)\n",
        "        print(f'Param combo {fold} complete, {round((time.time() - t0) / 60, 1)} elapsed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRCAfsrQsC78",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Do grid search\n",
        "do_grid_search(3, param_grid, end=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1IQGGGCmbnp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ieny3nV7lEP8"
      },
      "source": [
        "##**Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qj4pbLc7nMQq"
      },
      "outputs": [],
      "source": [
        "#@title Helper\n",
        "N_BAGGING_FOLDS = 100\n",
        "\n",
        "def do_pretrain_bagging(pretrain_loc, epochs, lrs, start=0, end=100):\n",
        "    out_dir = f'{DL_DATA_PATH}/bagging/pretrain_{pretrain_loc}_temperature2'\n",
        "    print('*****', out_dir)\n",
        "    ! mkdir $out_dir\n",
        "    test_years = [x for x in range(2017, END_YEAR_INC + 1)]\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    t0 = time.time()\n",
        "    for i in range(start, end):\n",
        "        pretrain_do = BaggingDataObj(feature_df, test_years, i + 1, ts_cols, flat_cols, y='evi_z')\n",
        "        full_do = BaggingDataObj(feature_df, test_years, i + 1, ts_cols, flat_cols)\n",
        "        preds = fit_predict_pretrain_fold(\n",
        "            pretrain_do, full_do, pretrain_loc, epochs, lrs, device, criterion, verbose=False)\n",
        "        preds.to_csv(f'{out_dir}/{i}.csv', index=False)\n",
        "\n",
        "        r = get_corr(preds['pred'], preds['log_yield'])\n",
        "        rmse = get_rmse(preds['pred'], preds['log_yield'])\n",
        "        print(f'Bagging fold {i + 1}/{N_BAGGING_FOLDS} complete, {round(time.time() - t0)} elapsed',\n",
        "                round(r, 3), round(rmse, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13q8_BFPfauf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Do bagging\n",
        "dir = f'{DL_DATA_PATH}/grid_search/pretrain_3_temperature.csv'\n",
        "params = pd.read_csv(dir).sort_values('rmse').iloc[0]\n",
        "\n",
        "epochs = (int(params['n_epochs_pt1']),\n",
        "          int(params['n_epochs_pt2']),\n",
        "          int(params['n_epochs_combined']))\n",
        "lrs = (params['lr1'], params['lr2'], params['lr_combo'])\n",
        "\n",
        "do_pretrain_bagging(3, epochs, lrs, start=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7PInWKZpxU_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLnyepVmyaKn"
      },
      "source": [
        "##**Fit in-sample models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEBVH-90NGqs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "years = [x for x in range(2000, END_YEAR_INC + 1)]\n",
        "pretrain_do = DataObj(feature_df, [2000], [2000], years, ts_cols, flat_cols, y=y_col)\n",
        "full_do = DataObj(feature_df, [2000], [2000], years, ts_cols, flat_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBZ2dG9_NGnq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Fit models\n",
        "pretrain_loc = 3\n",
        "dir = f'{DL_DATA_PATH}/grid_search/pretrain_{pretrain_loc}_temperature.csv'\n",
        "out_dir = f'{DL_DATA_PATH}/models/pretrain_3_temperature_unnorm/'\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "params = pd.read_csv(dir).sort_values('rmse').iloc[0]\n",
        "epochs = (int(params['n_epochs_pt1']),\n",
        "          int(params['n_epochs_pt2']),\n",
        "          int(params['n_epochs_combined']))\n",
        "lrs = (params['lr1'], params['lr2'], params['lr_combo'])\n",
        "\n",
        "t0 = time.time()\n",
        "for i in [0] + [x for x in range(2, 10)]:\n",
        "    _, model = fit_predict_pretrain_fold(pretrain_do, full_do, pretrain_loc, epochs, lrs, device,\n",
        "                                         criterion, verbose=False, return_model=True)\n",
        "    torch.save(model.state_dict(), f'{out_dir}/{i}_unnorm.pth')\n",
        "    print(i, '/ 10 complete,', round(time.time() - t0), 'elapsed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvlNc4dzPrx-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load models and predict\n",
        "m = HybridLSTM(use_best_params=True, use_ddays=False, read_params=False,\n",
        "               verbose=False, pretrain_loc=pretrain_loc, pretrain=False)\n",
        "m.load_state_dict(torch.load(f'{dir}/{i}.pth'))\n",
        "\n",
        "preds = test(m, device, full_do.train_x, full_do.train_y, criterion, predict=True)\n",
        "pred_df = full_do.train_pd[['year', 'fips', 'log_yield']].assign(\n",
        "    pred=full_do.ss_y.inverse_transform(preds.reshape(full_do.train_y.shape)).flatten())\n",
        "get_rmse(pred_df['log_yield'], pred_df['pred'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UXOiKqjR7VA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Gan7pByg7wRi",
        "qGMl031a8i4k",
        "M29XpBXaennI",
        "ex2ZQ59V-SNg",
        "qGT_8eRJNevK",
        "YQOgJCfnr8lN",
        "feLIkLnAmBPt",
        "hVRdJU_Ok8C9",
        "LnsnVTjFlGIT",
        "Ieny3nV7lEP8",
        "xQZDYuiAAVd6",
        "jg4Q-5TWzHth",
        "CjpFZO5EL1X_"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMym6UTco8ffgnuo5udJCWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}